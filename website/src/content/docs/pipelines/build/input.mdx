---
title: Input
sidebar:
    order: 2
---
import { Aside } from '@astrojs/starlight/components';
import { Tabs, TabItem } from '@astrojs/starlight/components';

An input is a source of data piped through an array of optional processors.

```yaml
input:
  label: my_redis_input
  redis_streams:
    url: tcp://localhost:6379
    streams:
      - wombat_stream
    body_key: body
    consumer_group: wombat_group

  # Optional list of processing steps
  processors:
    - mapping: |
        root.document = this.without("links")
        root.link_count = this.links.length()
```

Every Wombat pipeline has exactly one root input. However, this root input can be a `broker` which combines multiple
inputs and merges the streams:

```yaml
input:
  broker:
    inputs:
      - kafka:
          addresses: [ TODO ]
          topics: [ foo, bar ]
          consumer_group: foogroup

      - redis_streams:
          url: tcp://localhost:6379
          streams:
            - wombat_stream
          body_key: body
          consumer_group: wombat_group
```

Some inputs have a logical end, for example a `csv` input ends once the last row is consumed, when this
happens the input gracefully terminates and Wombat will shut itself down once all messages have been processed fully.

It's also possible to specify a logical end for an input that otherwise doesn't have one with the
`read_until` input, which checks a condition against each consumed message in order to determine
whether it should be the last.

## Scanners
For most Wombat inputs the data consumed comes pre-partitioned into discrete messages which can be
comfortably held and processed in memory. However, some inputs such as the `file` input often need to
consume data that is large enough that it cannot be processed entirely within memory, and others such as the
`socket` input don't have a concept of consuming the data "entirely".

For such inputs it's necessary to define a mechanism by which the stream of source bytes can be chopped into smaller
logical messages, processed and outputted as a continuous process whilst the stream is being read, as this dramatically
reduces the memory usage of Wombat as a whole and results in a more fluid flow of data.

The way in which we define this chopping mechanism is through scanners, configured as a field on each input that
requires one. For example, if we wished to consume files line-by-line, which each individual line being processed as a
discrete message, we could use the `lines` scanner with our `file` input:

<Tabs>
    <TabItem label="Common">
```yaml
input:
  file:
    paths: [ "./*.txt" ]
    scanner:
      lines: { }
```
    </TabItem>
    <TabItem label="Advanced">
```yaml
# Instead of newlines, use a custom delimiter:
input:
  file:
    paths: [ "./*.txt" ]
    scanner:
      lines:
        custom_delimiter: "---END---"
        max_buffer_size: 100_000_000 # 100MB line buffer
```
    </TabItem>
</Tabs>

A scanner is a plugin similar to any other core Wombat component (inputs, processors, outputs, etc), which means it's
possible to define your own scanners that can be utilised by inputs that need them.