---
title: sql_raw
---

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the corresponding source file under internal/impl/<provider>.
-->

Executes an arbitrary SQL query for each message.

Introduced in version 3.65.0.



## Examples
<tabs>
<tab title="Example 0">
	
Here we insert rows into a database by populating the columns id, name and topic with values extracted from messages and metadata:
	<code-block lang="yaml">
		
output:
  sql_raw:
    driver: mysql
    dsn: foouser:foopassword@tcp(localhost:3306)/foodb
    query: "INSERT INTO footable (id, name, topic) VALUES (?, ?, ?);"
    args_mapping: |
      root = [
        this.user.id,
        this.user.name,
        meta("kafka_topic"),
      ]

	</code-block>
</tab>

</tabs>


## Fields
| Field | Type | Default | Description |
|:-------|:------|:---------|:-------------|
| `driver` | `string` | `` | A database <<drivers, driver>> to use. |
| `dsn` | `string` | `` | A Data Source Name to identify the target database.

==== Drivers

:driver-support: mysql=certified, postgres=certified, clickhouse=community, mssql=community, sqlite=certified, oracle=certified, snowflake=community, trino=community, gocosmos=community

The following is a list of supported drivers, their placeholder style, and their respective DSN formats:

|===
| Driver | Data Source Name Format

| `clickhouse` 
| https://github.com/ClickHouse/clickhouse-go#dsn[`clickhouse://[username[:password\]@\][netloc\][:port\]/dbname[?param1=value1&...&paramN=valueN\]`^] 

| `mysql` 
| `[username[:password]@][protocol[(address)]]/dbname[?param1=value1&...&paramN=valueN]` 

| `postgres` 
| `postgres://[user[:password]@][netloc][:port][/dbname][?param1=value1&...]` 

| `mssql` 
| `sqlserver://[user[:password]@][netloc][:port][?database=dbname&param1=value1&...]` 

| `sqlite` 
| `file:/path/to/filename.db[?param&=value1&...]` 

| `oracle` 
| `oracle://[username[:password]@][netloc][:port]/service_name?server=server2&server=server3` 

| `snowflake` 
| `username[:password]@account_identifier/dbname/schemaname[?param1=value&...&paramN=valueN]` 

| `trino` 
| https://github.com/trinodb/trino-go-client#dsn-data-source-name[`http[s\]://user[:pass\]@host[:port\][?parameters\]`^] 

| `gocosmos` 
| https://pkg.go.dev/github.com/microsoft/gocosmos#readme-example-usage[`AccountEndpoint=<cosmosdb-endpoint>;AccountKey=<cosmosdb-account-key>[;TimeoutMs=<timeout-in-ms>\][;Version=<cosmosdb-api-version>\][;DefaultDb/Db=<db-name>\][;AutoId=<true/false>\][;InsecureSkipVerify=<true/false>\]`^] 
|===

Please note that the `postgres` driver enforces SSL by default, you can override this with the parameter `sslmode=disable` if required.

The `snowflake` driver supports multiple DSN formats. Please consult https://pkg.go.dev/github.com/snowflakedb/gosnowflake#hdr-Connection_String[the docs^] for more details. For https://docs.snowflake.com/en/user-guide/key-pair-auth.html#configuring-key-pair-authentication[key pair authentication^], the DSN has the following format: `<snowflake_user>@<snowflake_account>/<db_name>/<schema_name>?warehouse=<warehouse>&role=<role>&authenticator=snowflake_jwt&privateKey=<base64_url_encoded_private_key>`, where the value for the `privateKey` parameter can be constructed from an unencrypted RSA private key file `rsa_key.p8` using `openssl enc -d -base64 -in rsa_key.p8 | basenc --base64url -w0` (you can use `gbasenc` insted of `basenc` on OSX if you install `coreutils` via Homebrew). If you have a password-encrypted private key, you can decrypt it using `openssl pkcs8 -in rsa_key_encrypted.p8 -out rsa_key.p8`. Also, make sure fields such as the username are URL-encoded.

The https://pkg.go.dev/github.com/microsoft/gocosmos[`gocosmos`^] driver is still experimental, but it has support for https://learn.microsoft.com/en-us/azure/cosmos-db/hierarchical-partition-keys[hierarchical partition keys^] as well as https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/how-to-query-container#cross-partition-query[cross-partition queries^]. Please refer to the https://github.com/microsoft/gocosmos/blob/main/SQL.md[SQL notes^] for details. |
| `query` | `string` | `` | The query to execute. The style of placeholder to use depends on the driver, some drivers require question marks (`?`) whereas others expect incrementing dollar signs (`$1`, `$2`, and so on) or colons (`:1`, `:2` and so on). The style to use is outlined in this table:

| Driver | Placeholder Style |
|---|---|
| `clickhouse` | Dollar sign |
| `mysql` | Question mark |
| `postgres` | Dollar sign |
| `mssql` | Question mark |
| `sqlite` | Question mark |
| `oracle` | Colon |
| `snowflake` | Question mark |
| `trino` | Question mark |
| `gocosmos` | Colon | |
| `unsafe_dynamic_query` | `bool` | `false` | Whether to enable xref:configuration:interpolation.adoc#bloblang-queries[interpolation functions] in the query. Great care should be made to ensure your queries are defended against injection attacks. |
| `args_mapping` | `string` | `` | An optional xref:guides:bloblang/about.adoc[Bloblang mapping] which should evaluate to an array of values matching in size to the number of placeholder arguments in the field `query`. |
| `max_in_flight` | `int` | `64` | The maximum number of inserts to run in parallel. |
| `init_files` | `array` | `` | An optional list of file paths containing SQL statements to execute immediately upon the first connection to the target database. This is a useful way to initialise tables before processing data. Glob patterns are supported, including super globs (double star).

Care should be taken to ensure that the statements are idempotent, and therefore would not cause issues when run multiple times after service restarts. If both `init_statement` and `init_files` are specified the `init_statement` is executed _after_ the `init_files`.

If a statement fails for any reason a warning log will be emitted but the operation of this component will not be stopped. |
| `init_statement` | `string` | `` | An optional SQL statement to execute immediately upon the first connection to the target database. This is a useful way to initialise tables before processing data. Care should be taken to ensure that the statement is idempotent, and therefore would not cause issues when run multiple times after service restarts.

If both `init_statement` and `init_files` are specified the `init_statement` is executed _after_ the `init_files`.

If the statement fails for any reason a warning log will be emitted but the operation of this component will not be stopped. |
| `conn_max_idle_time` | `string` | `` | An optional maximum amount of time a connection may be idle. Expired connections may be closed lazily before reuse. If `value <= 0`, connections are not closed due to a connections idle time. |
| `conn_max_life_time` | `string` | `` | An optional maximum amount of time a connection may be reused. Expired connections may be closed lazily before reuse. If `value <= 0`, connections are not closed due to a connections age. |
| `conn_max_idle` | `int` | `2` | An optional maximum number of connections in the idle connection pool. If conn_max_open is greater than 0 but less than the new conn_max_idle, then the new conn_max_idle will be reduced to match the conn_max_open limit. If `value <= 0`, no idle connections are retained. The default max idle connections is currently 2. This may change in a future release. |
| `conn_max_open` | `int` | `` | An optional maximum number of open connections to the database. If conn_max_idle is greater than 0 and the new conn_max_open is less than conn_max_idle, then conn_max_idle will be reduced to match the new conn_max_open limit. If `value <= 0`, then there is no limit on the number of open connections. The default is 0 (unlimited). |
| `batching` | `object` | `` | Allows you to configure a xref:configuration:batching.adoc[batching policy]. |
| `batching.count` | `int` | `0` | A number of messages at which the batch should be flushed. If `0` disables count based batching. |
| `batching.byte_size` | `int` | `0` | An amount of bytes at which the batch should be flushed. If `0` disables size based batching. |
| `batching.period` | `string` | `""` | A period in which an incomplete batch should be flushed regardless of its size. |
| `batching.check` | `string` | `""` | A xref:guides:bloblang/about.adoc[Bloblang query] that should return a boolean value indicating whether a message should end a batch. |
| `batching.processors` | `array` | `` | A list of xref:components:processors/about.adoc[processors] to apply to a batch as it is flushed. This allows you to aggregate and archive the batch however you see fit. Please note that all resulting messages are flushed as a single batch, therefore splitting the batch into smaller batches using these processors is a no-op. |

